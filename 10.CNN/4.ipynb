{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迁移学习和微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**迁移学习** 包含对一个问题学习的特征，以及利用他们解决一个新的、类似的问题。例如，具有学会识别浣熊可能对启动一个用来识别的模型很有用。\n",
    "迁移学习通常用于数据集数据量太少的任务从头开始训练全尺寸模型。\n",
    "在深度学习的背景下，迁移学习最常见的体现是遵循工作流:\n",
    "1. 从先前训练过的模型中提取图层。\n",
    "2. 冻结它们，以避免在以后的训练中破坏它们所包含的任何信息。\n",
    "3. 在冻结层的顶部添加一些新的、可训练的层。他们将学习如何将旧的特性转化为对新数据集的预测。\n",
    "4. 在数据集中训练新层。\n",
    "最后一个可选步骤是**微调**，它包括解冻上面获得的整个模型（或其中的一部分），然后以非常低的学习率在新数据上重新训练它。通过逐步地将预训练的特性适应新数据，这可能会实现有意义的改进。\n",
    "首先，我们将详细介绍Keras“trainable”API，它是大多数迁移学习和微调工作流的基础。\n",
    "然后，我们将演示典型的工作流，方法是在ImageNet数据集上预先训练一个模型，然后在Kaggle“猫对狗”分类数据集上对其进行再训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 冻结层：了解`trainable`属性\n",
    "层和模型有三个权重属性：\n",
    "- `weights` 是层的所有权重变量的列表。\n",
    "- `trainable_weights` 是要更新（通过梯度下降）以将训练过程中的损失最小化的列表。\n",
    "- `non_trainable_weights` 是那些不需要训练的对象的名单。\n",
    " 通常，在前向传递过程中，模型会对其进行更新。\n",
    " \n",
    "**示例：`Dense` 层有2个可训练权重（内核和偏移）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(3)\n",
    "layer.build((None, 4))  # Create the weights\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般来说，所有重量都是可训练的重量。唯一具有不可训练权重的内置层是`BatchNormalization`层。它使用不可训练的权值来跟踪训练期间输入的均值和方差。\n",
    "要了解如何在自己的自定义层中使用不可训练的权重，请参见\n",
    "[从头开始编写新图层指南](https://keras.io/guides/making_new_layers_and_models_via_subclassing/).\n",
    "\n",
    "**示例：`BatchNormalization`层有2个可训练权重和2个不可训练权重**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.BatchNormalization()\n",
    "layer.build((None, 4))  # Create the weights\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers&models还具有布尔属性`trainable`。它的值可以更改。\n",
    "\n",
    "设置`layer.trainable`为 `False` 将层的所有权重从可训练移动到不可训练。这被称为“冻结”层：冻结层的状态在训练期间不会更新（无论是在使用`fit()`进行训练时，还是在使用任何依赖于`trainable_weights`应用渐变更新的自定义循环进行训练时）。\n",
    "\n",
    "**示例: setting `trainable` to `False`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(3)\n",
    "layer.build((None, 4))  # Create the weights\n",
    "layer.trainable = False  # Freeze the layer\n",
    "\n",
    "print(\"weights:\", len(layer.weights))\n",
    "print(\"trainable_weights:\", len(layer.trainable_weights))\n",
    "print(\"non_trainable_weights:\", len(layer.non_trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当可训练权重变为不可训练时，其值在训练期间不再更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a model with 2 layers\n",
    "layer1 = keras.layers.Dense(3, activation=\"relu\")\n",
    "layer2 = keras.layers.Dense(3, activation=\"sigmoid\")\n",
    "model = keras.Sequential([keras.Input(shape=(3, )), layer1, layer2])\n",
    "\n",
    "# Freeze the first layer\n",
    "layer1.trainable = False\n",
    "\n",
    "# 保留layer1的权重副本以供以后参考\n",
    "initial_layer1_weights_values = layer1.get_weights()\n",
    "\n",
    "# Train the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))\n",
    "\n",
    "# 检查第1层的重量在训练期间没有变化\n",
    "final_layer1_weights_values = layer1.get_weights()\n",
    "np.testing.assert_allclose(initial_layer1_weights_values[0],\n",
    "                           final_layer1_weights_values[0])\n",
    "np.testing.assert_allclose(initial_layer1_weights_values[1],\n",
    "                           final_layer1_weights_values[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不要混淆`layer.trainable` 在`layer.__call__()`中具有参数`training`的属性（它控制层是在推理模式还是训练模式下运行前向传递）。有关详细信息，请参阅\n",
    "[Keras FAQ](\n",
    "  https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `rainable`属性的递归设置\n",
    "如果在模型或具有子层的任何层上设置`trainable = False` ，则所有子层也将变为不可训练。\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner_model = keras.Sequential([\n",
    "    keras.Input(shape=(3, )),\n",
    "    keras.layers.Dense(3, activation=\"relu\"),\n",
    "    keras.layers.Dense(3, activation=\"relu\"),\n",
    "])\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.Input(shape=(3, )),\n",
    "    inner_model,\n",
    "    keras.layers.Dense(3, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "model.trainable = False  # 冻结外部模型\n",
    "\n",
    "assert inner_model.trainable == False  # “model”中的所有层现在都被冻结了\n",
    "assert inner_model.layers[0].trainable == False  # `trainable` 以递归方式传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The typical transfer-learning workflow\n",
    "This leads us to how a typical transfer learning workflow can be implemented in Keras:\n",
    "1. Instantiate a base model and load pre-trained weights into it.\n",
    "2. Freeze all layers in the base model by setting `trainable = False`.\n",
    "3. Create a new model on top of the output of one (or several) layers from the base\n",
    " model.\n",
    "4. Train your new model on your new dataset.\n",
    "Note that an alternative, more lightweight workflow could also be:\n",
    "1. Instantiate a base model and load pre-trained weights into it.\n",
    "2. Run your new dataset through it and record the output of one (or several) layers\n",
    " from the base model. This is called **feature extraction**.\n",
    "3. Use that output as input data for a new, smaller model.\n",
    "A key advantage of that second workflow is that you only run the base model once on\n",
    " your data, rather than once per epoch of training. So it's a lot faster & cheaper.\n",
    "An issue with that second workflow, though, is that it doesn't allow you to dynamically\n",
    "modify the input data of your new model during training, which is required when doing\n",
    "data augmentation, for instance. Transfer learning is typically used for tasks when\n",
    "your new dataset has too little data to train a full-scale model from scratch, and in\n",
    "such scenarios data augmentation is very important. So in what follows, we will focus\n",
    " on the first workflow.\n",
    "Here's what the first workflow looks like in Keras:\n",
    "First, instantiate a base model with pre-trained weights.\n",
    "```python\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top.\n",
    "```\n",
    "Then, freeze the base model.\n",
    "```python\n",
    "base_model.trainable = False\n",
    "```\n",
    "Create a new model on top.\n",
    "```python\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "# We make sure that the base_model is running in inference mode here,\n",
    "# by passing `training=False`. This is important for fine-tuning, as you will\n",
    "# learn in a few paragraphs.\n",
    "x = base_model(inputs, training=False)\n",
    "# Convert features of shape `base_model.output_shape[1:]` to vectors\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "# A Dense classifier with a single unit (binary classification)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "```\n",
    "Train the model on new data.\n",
    "```python\n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "model.fit(new_dataset, epochs=20, callbacks=..., validation_data=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "Once your model has converged on the new data, you can try to unfreeze all or part of\n",
    " the base model and retrain the whole model end-to-end with a very low learning rate.\n",
    "This is an optional last step that can potentially give you incremental improvements.\n",
    " It could also potentially lead to quick overfitting -- keep that in mind.\n",
    "It is critical to only do this step *after* the model with frozen layers has been\n",
    "trained to convergence. If you mix randomly-initialized trainable layers with\n",
    "trainable layers that hold pre-trained features, the randomly-initialized layers will\n",
    "cause very large gradient updates during training, which will destroy your pre-trained\n",
    " features.\n",
    "It's also critical to use a very low learning rate at this stage, because\n",
    "you are training a much larger model than in the first round of training, on a dataset\n",
    " that is typically very small.\n",
    "As a result, you are at risk of overfitting very quickly if you apply large weight\n",
    " updates. Here, you only want to readapt the pretrained weights in an incremental way.\n",
    "This is how to implement fine-tuning of the whole base model:\n",
    "```python\n",
    "# Unfreeze the base model\n",
    "base_model.trainable = True\n",
    "# It's important to recompile your model after you make any changes\n",
    "# to the `trainable` attribute of any inner layer, so that your changes\n",
    "# are take into account\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5),  # Very low learning rate\n",
    "              loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])\n",
    "# Train end-to-end. Be careful to stop before you overfit!\n",
    "model.fit(new_dataset, epochs=10, callbacks=..., validation_data=...)\n",
    "```\n",
    "**Important note about `compile()` and `trainable`**\n",
    "Calling `compile()` on a model is meant to \"freeze\" the behavior of that model. This\n",
    " implies that the `trainable`\n",
    "attribute values at the time the model is compiled should be preserved throughout the\n",
    " lifetime of that model,\n",
    "until `compile` is called again. Hence, if you change any `trainable` value, make sure\n",
    " to call `compile()` again on your\n",
    "model for your changes to be taken into account.\n",
    "**Important notes about `BatchNormalization` layer**\n",
    "Many image models contain `BatchNormalization` layers. That layer is a special case on\n",
    " every imaginable count. Here are a few things to keep in mind.\n",
    "- `BatchNormalization` contains 2 non-trainable weights that get updated during\n",
    "training. These are the variables tracking the mean and variance of the inputs.\n",
    "- When you set `bn_layer.trainable = False`, the `BatchNormalization` layer will\n",
    "run in inference mode, and will not update its mean & variance statistics. This is not\n",
    "the case for other layers in general, as\n",
    "[weight trainability & inference/training modes are two orthogonal concepts](\n",
    "  https://keras.io/getting_started/faq/#whats-the-difference-between-the-training-argument-in-call-and-the-trainable-attribute).\n",
    "But the two are tied in the case of the `BatchNormalization` layer.\n",
    "- When you unfreeze a model that contains `BatchNormalization` layers in order to do\n",
    "fine-tuning, you should keep the `BatchNormalization` layers in inference mode by\n",
    " passing `training=False` when calling the base model.\n",
    "Otherwise the updates applied to the non-trainable weights will suddenly destroy\n",
    "what the model has learned.\n",
    "You'll see this pattern in action in the end-to-end example at the end of this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迁移学习和使用定制训练循环进行微调\n",
    "如果不是使用`fit()`，而是使用自己的低级培训循环，那么工作流基本上保持不变。你应该小心只考虑清单`model.trainable_weights`应用渐变更新时：\n",
    "```python\n",
    "# 创建基础模型\n",
    "base_model = keras.applications.Xception(\n",
    "    weights='imagenet',\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False)\n",
    "# 冻结 base model\n",
    "base_model.trainable = False\n",
    "# 在顶部创建新模型。\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optimizer = keras.optimizers.Adam()\n",
    "# 迭代数据集的批处理。\n",
    "for inputs, targets in new_dataset:\n",
    "    # 打开渐变带。\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 向前传播。\n",
    "        predictions = model(inputs)\n",
    "        # 计算此批次的损失值。\n",
    "        loss_value = loss_fn(targets, predictions)\n",
    "    # 得到与可训练重量相关的损失梯度。\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    #更新模型的权重。\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "微调也是如此。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个端到端的例子：对猫和狗的图像分类模型进行微调\n",
    " 为了巩固这些概念，让我们介绍一个具体的端到端转移学习和微调示例。我们将加载在ImageNet上预先训练过的异常模型，并将其用于Kaggle“cats vs.dogs”分类数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据\n",
    "首先，让我们使用TFDS获取cats和dogs数据集。如果您有自己的数据集，您可能希望使用该实用程序`tf.keras.preprocessing.image_dataset_from_directory`从磁盘上的一组映像生成类似的带标签的数据集对象，并将其放入类特定的文件夹中。当使用非常小的数据集时，迁移学习是最有用的。为了保持数据集的小，我们将使用40%的原始训练数据（25000幅图像）进行训练，10%用于验证，10%用于测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "train_ds, validation_ds, test_ds = tfds.load(\n",
    "    \"cats_vs_dogs\",\n",
    "    # 保留10%用于验证，10%用于测试\n",
    "    split=[\"train[:40%]\", \"train[40%:50%]\", \"train[50%:60%]\"],\n",
    "    as_supervised=True,  # 包括标签\n",
    ")\n",
    "\n",
    "print(\"Number of training samples: %d\" %\n",
    "      tf.data.experimental.cardinality(train_ds))\n",
    "print(\"Number of validation samples: %d\" %\n",
    "      tf.data.experimental.cardinality(validation_ds))\n",
    "print(\"Number of test samples: %d\" % tf.data.experimental.cardinality(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是训练数据集中的前9张图片——如你所见，它们的大小都不一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i, (image, label) in enumerate(train_ds.take(9)):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(int(label))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以看到标签1是“狗”，标签0是“猫”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据标准化\n",
    "我们的原始图像有各种尺寸。此外，每个像素由3个介于0和255之间的整数值（RGB级别的值）组成。这对神经网络来说不太合适。我们需要做两件事:\n",
    "- 标准化为固定的图像大小。我们选150x150。\n",
    "- 规格化-1和1之间的像素值。我们将使用“Normalization”层作为模型本身的一部分来实现这一点。\n",
    "一般来说，开发以原始数据作为输入的模型是一个好的实践，而不是使用已经预处理的数据的模型。原因是，如果您的模型需要预处理的数据，那么每当您将模型导出到其他地方（在web浏览器中、在移动应用程序中）使用它，您都需要重新实现完全相同的预处理管道。这很快就会变得非常棘手。所以我们应该在撞击模型之前做尽可能少的预处理。在这里，我们将在数据管道中调整图像大小（因为深度神经网络只能处理连续的成批数据），当我们创建模型时，我们会将输入值缩放作为模型的一部分。让我们将图像大小调整为150x150："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (150, 150)\n",
    "\n",
    "train_ds = train_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "validation_ds = validation_ds.map(lambda x, y: (tf.image.resize(x, size), y))\n",
    "test_ds = test_ds.map(lambda x, y: (tf.image.resize(x, size), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，让我们批处理数据并使用缓存和预取来优化加载速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_ds = train_ds.cache().batch(batch_size).prefetch(buffer_size=10)\n",
    "validation_ds = validation_ds.cache().batch(batch_size).prefetch(\n",
    "    buffer_size=10)\n",
    "test_ds = test_ds.cache().batch(batch_size).prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机数据扩充\n",
    "当您没有大型图像数据集时，通过对训练图像应用随机但真实的变换（如随机水平翻转或小的随机旋转）来人为地引入样本多样性是一个很好的实践。这有助于将模型暴露于训练数据的不同方面，同时减缓过度拟合的速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们想象一下，在各种随机操作之后，第一批的第一个图像是什么样子的转换："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for images, labels in train_ds.take(1):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    first_image = images[0]\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0),\n",
    "                                            training=True)\n",
    "        plt.imshow(augmented_image[0].numpy().astype(\"int32\"))\n",
    "        plt.title(int(labels[i]))\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型\n",
    "现在，让我们按照前面解释的蓝图构建一个模型。\n",
    "\n",
    "请注意:\n",
    "- 我们添加一个`Normalization`层来缩放输入值（最初在 `[0, 255]` 范围）到 `[-1, 1]` 范围。\n",
    "- 为了正则化，我们在分类层之前添加了一个`Dropout`层。\n",
    "- 我们确保在调用基本模型时传递`training=False`，以便它在推理模式下运行，这样即使在我们解冻基础模型进行微调之后，batchnorm统计信息也不会更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.Xception(\n",
    "    weights=\"imagenet\",  # 在ImageNet上预先训练的权重。\n",
    "    input_shape=(150, 150, 3),\n",
    "    include_top=False,\n",
    ")  # 不要在顶部包含ImageNet分类器。\n",
    "\n",
    "# 冻结base_model\n",
    "base_model.trainable = False\n",
    "\n",
    "# 在顶部创建新模型\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "x = data_augmentation(inputs)  # 应用随机数据扩充\n",
    "\n",
    "#预先训练的Xception权重要求输入标准化\n",
    "#从（0，255）到范围（-1，+1.），标准化层\n",
    "#以下是输出=（输入-平均值）/sqrt（var）\n",
    "norm_layer = keras.layers.experimental.preprocessing.Normalization()\n",
    "mean = np.array([127.5] * 3)\n",
    "var = mean**2\n",
    "#将输入缩放到[-1，+1]\n",
    "x = norm_layer(x)\n",
    "norm_layer.set_weights([mean, var])\n",
    "\n",
    "#基本模型包含batchnorm层。我们想让他们保持推理模式\n",
    "\n",
    "#当我们解冻基础模型进行微调时，我们确保\n",
    "\n",
    "#base_model在这里以推理模式运行。\n",
    "x = base_model(x, training=False)\n",
    "x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "x = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\n",
    "outputs = keras.layers.Dense(1)(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练顶层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 20\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对整个模型进行一轮微调\n",
    "最后，让我们解冻基本模型，并以较低的学习率对整个模型进行端到端的训练。\n",
    "重要的是，尽管基本模型可以训练，但它仍然在推理模式下运行，因为我们在构建模型时调用了`training=False`。这意味着内部的批处理规范化层不会更新其批处理统计信息。如果他们这样做了，他们将破坏模型迄今为止所学的表示法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解冻基本模型。请注意，它一直在推理模式下运行，因为我们在调用它时通过了`training=False`。这意味着batchnorm层不会更新其批处理统计信息。这可以防止batchnorm层取消我们到目前为止所做的所有训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),  # 低学习率\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    metrics=[keras.metrics.BinaryAccuracy()],\n",
    ")\n",
    "\n",
    "epochs = 10\n",
    "model.fit(train_ds, epochs=epochs, validation_data=validation_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过10个epochs，微调为我们带来了一个很好的改善。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
